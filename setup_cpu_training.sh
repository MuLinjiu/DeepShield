#!/bin/bash
# CPUè®­ç»ƒé…ç½®å’Œå¯åŠ¨è„šæœ¬

set -e

echo "=================================================="
echo "DeepShield LoRA - CPUä¼˜åŒ–ç‰ˆæœ¬è®­ç»ƒ"
echo "=================================================="

GREEN='\033[0;32m'
YELLOW='\033[1;33m'
RED='\033[0;31m'
NC='\033[0m'

# æ­¥éª¤1: å®‰è£…ä¾èµ–
install_dependencies() {
    echo -e "\n${GREEN}[1/4] å®‰è£…CPUç‰ˆæœ¬ä¾èµ–...${NC}"
    echo "è¿™å°†å®‰è£…çº¯CPUç‰ˆæœ¬çš„PyTorchï¼ˆæ— CUDAæ”¯æŒï¼‰"
    
    # æ£€æŸ¥pip
    if ! python3 -m pip --version &>/dev/null; then
        echo "å®‰è£… pip..."
        sudo apt update
        sudo apt install -y python3-pip
    fi
    
    # å®‰è£…CPUç‰ˆæœ¬çš„PyTorch
    echo "å®‰è£…PyTorch (CPUç‰ˆæœ¬)..."
    python3 -m pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu
    
    # å®‰è£…å…¶ä»–ä¾èµ–
    echo "å®‰è£…å…¶ä»–ä¾èµ–..."
    python3 -m pip install transformers datasets peft accelerate scipy numpy tqdm sentencepiece protobuf
    
    echo -e "${GREEN}âœ“ ä¾èµ–å®‰è£…å®Œæˆ${NC}"
}

# æ­¥éª¤2: éªŒè¯å®‰è£…
verify_installation() {
    echo -e "\n${GREEN}[2/4] éªŒè¯å®‰è£…...${NC}"
    
    python3 -c "
import torch
import transformers
import datasets
import peft

print('âœ“ PyTorch version:', torch.__version__)
print('âœ“ Transformers version:', transformers.__version__)
print('âœ“ Datasets version:', datasets.__version__)
print('âœ“ PEFT version:', peft.__version__)
print('âœ“ CUDA available:', torch.cuda.is_available())
print('âœ“ CPU threads:', torch.get_num_threads())

if torch.cuda.is_available():
    print('âš ï¸  æ£€æµ‹åˆ°GPUï¼Œä½†æˆ‘ä»¬å°†ä½¿ç”¨CPUç‰ˆæœ¬è®­ç»ƒ')
else:
    print('âœ“ å°†ä½¿ç”¨CPUè¿›è¡Œè®­ç»ƒï¼ˆé€Ÿåº¦è¾ƒæ…¢ä½†å¯è¡Œï¼‰')
"
    
    if [ $? -eq 0 ]; then
        echo -e "${GREEN}âœ“ çŽ¯å¢ƒéªŒè¯é€šè¿‡${NC}"
    else
        echo -e "${RED}âœ— çŽ¯å¢ƒéªŒè¯å¤±è´¥${NC}"
        exit 1
    fi
}

# æ­¥éª¤3: åˆ›å»ºå°æ•°æ®é›†
prepare_small_dataset() {
    echo -e "\n${GREEN}[3/4] å‡†å¤‡è®­ç»ƒæ•°æ®...${NC}"
    echo "ä¸ºäº†åœ¨CPUä¸Šåˆç†çš„æ—¶é—´å†…å®Œæˆè®­ç»ƒï¼Œæˆ‘ä»¬å°†ä½¿ç”¨è¾ƒå°çš„æ•°æ®é›†"
    
    # åˆ›å»ºå°æ•°æ®é›†ï¼ˆå‡å°‘åˆ°100æ¡é¿å…å†…å­˜ä¸è¶³ï¼‰
    head -n 100 data/processed/llm_input_enriched_train.jsonl > data/cpu_train_1k.jsonl
    head -n 20 data/processed/llm_input_enriched_val.jsonl > data/cpu_val_200.jsonl
    head -n 20 data/processed/llm_input_enriched_test.jsonl > data/cpu_test_100.jsonl
    
    echo -e "${GREEN}âœ“ æ•°æ®é›†å‡†å¤‡å®Œæˆ${NC}"
    echo "  - è®­ç»ƒé›†: 100 æ¡è®°å½•ï¼ˆå†…å­˜ä¼˜åŒ–ï¼‰"
    echo "  - éªŒè¯é›†: 20 æ¡è®°å½•"
    echo "  - æµ‹è¯•é›†: 20 æ¡è®°å½•"
}

# æ­¥éª¤4: å¼€å§‹è®­ç»ƒ
start_training() {
    echo -e "\n${GREEN}[4/4] å¼€å§‹CPUè®­ç»ƒ...${NC}"
    echo "é…ç½®ä¿¡æ¯ï¼š"
    echo "  - æ¨¡åž‹: TinyLlama-1.1B (å°åž‹æ¨¡åž‹ï¼ŒCPUå‹å¥½)"
    echo "  - è®­ç»ƒæ ·æœ¬: 100æ¡ï¼ˆå†…å­˜ä¼˜åŒ–ï¼‰"
    echo "  - åºåˆ—é•¿åº¦: 512 (å¤§å¹…å‡å°‘å†…å­˜)"
    echo "  - Batch size: 1"
    echo "  - è®­ç»ƒè½®æ•°: 2"
    echo ""
    echo -e "${YELLOW}âš ï¸  CPUè®­ç»ƒé¢„è®¡éœ€è¦æ•°å°æ—¶ï¼Œè¯·è€å¿ƒç­‰å¾…${NC}"
    echo ""
    
    mkdir -p lora-netflow-cpu
    
    # CPUä¼˜åŒ–è®­ç»ƒï¼ˆæžåº¦å†…å­˜ä¼˜åŒ–ï¼‰
    python3 train_cpu_optimized.py \
        --train_path data/cpu_train_1k.jsonl \
        --val_path data/cpu_val_200.jsonl \
        --out_dir lora-netflow-cpu \
        --base_model TinyLlama/TinyLlama-1.1B-Chat-v1.0 \
        --epochs 2 \
        --per_device_bs 1 \
        --grad_accum 2 \
        --lr 3e-4 \
        --max_len 512 \
        --r 4 \
        --alpha 8
    
    if [ $? -eq 0 ]; then
        echo -e "\n${GREEN}=================================================="
        echo "ðŸŽ‰ è®­ç»ƒå®Œæˆï¼"
        echo "=================================================="
        echo "æ¨¡åž‹å·²ä¿å­˜åˆ°: lora-netflow-cpu/"
        echo ""
        echo "è¯„ä¼°æ¨¡åž‹:"
        echo "python3 train_cpu_optimized.py \\"
        echo "    --eval_path data/cpu_test_100.jsonl \\"
        echo "    --adapter lora-netflow-cpu --mode eval"
        echo "=================================================="
        echo -e "${NC}"
    else
        echo -e "\n${RED}è®­ç»ƒå‡ºé”™${NC}"
        exit 1
    fi
}

# ä¸»æµç¨‹
main() {
    cd /home/ubuntu/Workspace/DeepShield
    
    if [ "$1" == "--skip-install" ]; then
        echo "è·³è¿‡å®‰è£…ï¼Œç›´æŽ¥è®­ç»ƒ..."
        prepare_small_dataset
        start_training
    elif [ "$1" == "--quick-test" ]; then
        echo "å¿«é€Ÿæµ‹è¯•æ¨¡å¼ï¼ˆä»…10æ¡æ•°æ®ï¼‰..."
        head -n 10 data/processed/llm_input_enriched_train.jsonl > data/cpu_test_10.jsonl
        python3 train_cpu_optimized.py \
            --train_path data/cpu_test_10.jsonl \
            --val_path data/cpu_test_10.jsonl \
            --out_dir lora-cpu-test \
            --epochs 1 \
            --max_len 512
    else
        install_dependencies
        verify_installation
        prepare_small_dataset
        start_training
    fi
}

main "$@"
